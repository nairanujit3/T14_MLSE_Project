# T14_MLSE_Project

Anujit Nair - 202418036

Vaibhav Agrawal - 202418059

# ğŸš€ Multimodal RAG System (CLIP + FAISS + Ollama)

This project implements a **fully local, free, open-source multimodal Retrieval-Augmented Generation (RAG) system** that supports:

### âœ… **Text â†’ Image Retrieval**  
Enter a prompt â†’ CLIP retrieves the most relevant image from your dataset.

### âœ… **Image â†’ Text Generation**  
Upload an image â†’ CLIP retrieves the closest caption â†’ Ollama LLM (Gemma-3 4B) expands it into a **creative paragraph**.

### â­ 100% Offline  
- CLIP ViT-B/32 for image & text embeddings  
- FAISS for fast similarity search  
- Ollama (Gemma-3:4B) for creative text generation  
- Works on CPU/GPU

---

## ğŸ“ Project Structure
```
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ images/ # Dataset images
â”‚ â””â”€â”€ dataset.csv # Image paths + captions (image_path, caption)
â”‚
â”œâ”€â”€ indices/
â”‚ â”œâ”€â”€ image_index.faiss # FAISS index for image embeddings
â”‚ â”œâ”€â”€ text_index.faiss # FAISS index for text embeddings
â”‚ â”œâ”€â”€ image_embeddings.npy
â”‚ â”œâ”€â”€ text_embeddings.npy
â”‚ â””â”€â”€ metadata.json # Stores image paths + captions
â”‚
â”œâ”€â”€ rag_env/ # Python virtual environment
â”‚
â”œâ”€â”€ app.py # Streamlit Web App
â”œâ”€â”€ prepare_index.py # Builds FAISS indices (one-time)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---

## âš™ï¸ Installation

### 1. Clone the repository

```bash
git clone <your-repo-url>
cd <repo-folder>
```

### 2. Create & activate virtual environment
```
python -m venv rag_env
.\rag_env\Scripts\activate    # Windows
```

### 3.Install dependencies
```
pip install -r requirements.txt

```
### 4. Install & run Ollama
Download Ollama: https://ollama.com/download

Then pull the model:
```
ollama pull gemma3:4b
ollama serve
```

### ğŸ–¥ï¸ Running the Streamlit App
```
streamlit run app.py
```

Features:
- Text â†’ Image (Create Image)
  
    - Enter a prompt
    - CLIP retrieves the closest image
    - Only the final matched image is shown
    (no captions, no similarity scores)

- Image â†’ Text (Generate Text)
  
    - Upload an image
    - CLIP retrieves the closest caption
    - Ollama turns it into a creative, story-like paragraph
    (no dataset images or captions displayed)

### ğŸ§  Architecture (How It Works)
      Text Query                           Image Input
          â”‚                                     â”‚
          â–¼                                     â–¼
      CLIP Text Embed                      CLIP Image Embed
          â”‚                                     â”‚
          â–¼                                     â–¼
      FAISS Search                         FAISS Search
          â”‚                                     â”‚
          â–¼                                     â–¼
  Best Matching Image                Best Matching Caption
          â”‚                                     â”‚
          â–¼                                     â–¼
     Final Output                    Ollama LLM (Gemma3 4B)
                                               â”‚
                                               â–¼
                             Creative Image Description Paragraph

### ğŸš€ Future Improvements

- Fine-tuned CLIP model for higher accuracy
- Add stable diffusion or SDXL for generating images
- Multi-image retrieval
- FastAPI backend + React/Flutter front-end
- GPU-accelerated inference


<video src="[https://github.com/user-attachments/assets/ea6a4f6b-8e4f-4022-8bfa-2e0f9b2e701e]" controls title="Video Title"></video>



